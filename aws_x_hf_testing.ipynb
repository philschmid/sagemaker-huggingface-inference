{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "automotive-destiny",
   "metadata": {},
   "source": [
    "# Huggingface Sagemaker-sdk - Deploy ðŸ¤— Transformers for inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-frost",
   "metadata": {},
   "source": [
    "1. [Introduction](#Introduction)  \n",
    "2. [Deploy a trained Hugging Face Transformer model on to SageMaker for inference](#Deploy-a-trained-Hugging-Face-Transformer-model-on-to-SageMaker-for-inference)  \n",
    "    a. [Deploy the model directly after training](#Deploy-the-model-directly-after-training)  \n",
    "    b. [Deploy the model using `model_data`](#Deploy-the-model-using-model_data)  \n",
    "3. [Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference](#Deploy-one-of-the-10-000+-Hugging-Face-Transformers-to-Amazon-SageMaker-for-Inference)   \n",
    "\n",
    "\n",
    "Welcome to this getting started guide, we will use the new Hugging Face Inference DLCs and Amazon SageMaker Python SDK to deploy two transformer model for inference. \n",
    "In the first example we deploy a trained Hugging Face Transformer model on to SageMaker for inference.\n",
    "In the second example we directly deploy one of the 10 000+ Hugging Face Transformers from the [Hub](https://huggingface.co/models) to Amazon SageMaker for Inference.\n",
    "\n",
    "You can find the documentation for the inference solution [here](add link)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-enclosure",
   "metadata": {},
   "source": [
    "## API - [SageMaker Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-xerox",
   "metadata": {},
   "source": [
    "Using the `transformers pipelines`, we designed an API, which makes it easy for you to benefit from all `pipelines` features. The API is oriented at the API of the [ðŸ¤—  Accelerated Inference API](https://api-inference.huggingface.co/docs/python/html/detailed_parameters.html), meaning your inputs need to be defined in the `inputs` key and if you want additional supported `pipelines` parameters you can add them in the `parameters` key. Below you can find examples for requests. \n",
    "\n",
    "**text-classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "```\n",
    "**question-answering request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": {\n",
    "\t\t\"question\": \"What is used for inference?\",\n",
    "\t\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "```\n",
    "**zero-shot classification request body**\n",
    "```python\n",
    "{\n",
    "\t\"inputs\": \"Hi, I recently bought a device from your company but it is not working as advertised and I would like to get reimbursed!\",\n",
    "\t\"parameters\": {\n",
    "\t\t\"candidate_labels\": [\n",
    "\t\t\t\"refund\",\n",
    "\t\t\t\"legal\",\n",
    "\t\t\t\"faq\"\n",
    "\t\t]\n",
    "\t}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-prospect",
   "metadata": {},
   "source": [
    "## Deep Learning Container\n",
    "\n",
    "TODO: Update to correct containers\n",
    "\n",
    "PyTorch:  \n",
    "CPU - `801740330924.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch:1.7-cpu`  \n",
    "GPU - `801740330924.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch:1.7-gpu`\n",
    "\n",
    "TensorFlow:  \n",
    "CPU - `801740330924.dkr.ecr.us-west-2.amazonaws.com/huggingface-tensorflow:2.4-cpu`  \n",
    "GPU - `801740330924.dkr.ecr.us-west-2.amazonaws.com/huggingface-tensorflow:2.4-gpu`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch\n",
    "pytorch_cpu_image_uri=\"558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-inference-pytorch:0.0.1-cpu\"\n",
    "pytorch_gpu_image_uri=\"558105141721.dkr.ecr.us-east-1.amazonaws.com/huggingface-inference-pytorch:0.0.1-gpu\"\n",
    "\n",
    "#tensorflow\n",
    "tensorflow_cpu_image_uri=\"\"\n",
    "tensorflow_gpu_image_uri=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-pulse",
   "metadata": {},
   "source": [
    "## Models table\n",
    "\n",
    "| model                                                 | task                     | framework  | link                                                                          |\n",
    "|-------------------------------------------------------|--------------------------|------------|-------------------------------------------------------------------------------|\n",
    "| distilbert-base-uncased-finetuned-sst-2-english       | text-classification      | PyTorch    | https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english        |\n",
    "| distilbert-base-uncased-finetuned-sst-2-english       | text-classification      | TensorFlow | https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english        |\n",
    "| joeddav/xlm-roberta-large-xnli                        | zero-shot-classification | PyTorch    | https://huggingface.co/joeddav/xlm-roberta-large-xnli                         |\n",
    "| bert-base-uncased                                     | feature-extraction       | PyTorch    | https://huggingface.co/bert-base-uncased                                      |\n",
    "| bert-base-uncased                                     | feature-extraction       | TensorFlow | https://huggingface.co/bert-base-uncased                                      |\n",
    "| dbmdz/bert-large-cased-finetuned-conll03-english      | ner                      | PyTorch    | https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english       |\n",
    "| dbmdz/bert-large-cased-finetuned-conll03-english      | ner                      | TensorFlow | https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english       |\n",
    "| distilbert-base-uncased-distilled-squad               | question-answering       | PyTorch    | https://huggingface.co/distilbert-base-uncased-distilled-squad                |\n",
    "| distilbert-base-uncased-distilled-squad               | question-answering       | TensorFlow | https://huggingface.co/distilbert-base-uncased-distilled-squad                |\n",
    "| albert-base-v2                                        | fill-mask                | PyTorch    | https://huggingface.co/albert-base-v2                                         |\n",
    "| albert-base-v2                                        | fill-mask                | TensorFlow | https://huggingface.co/albert-base-v2                                         |\n",
    "| sshleifer/distilbart-cnn-12-6                         | summarization            | PyTorch    | https://huggingface.co/sshleifer/distilbart-cnn-12-6                          |\n",
    "| google/pegasus-xsum                                   | summarization            | TensorFlow | https://huggingface.co/google/pegasus-xsum                                    |\n",
    "| Helsinki-NLP/opus-mt-en-de                            | translation_xx_to_yy     | PyTorch    | https://huggingface.co/Helsinki-NLP/opus-mt-en-de                             |\n",
    "| Helsinki-NLP/opus-mt-en-de                            | translation_xx_to_yy     | TensorFlow | https://huggingface.co/Helsinki-NLP/opus-mt-en-de                             |\n",
    "| t5-base                                               | text2text-generation     | PyTorch    | https://huggingface.co/t5-base                                                |\n",
    "| t5-base                                               | text2text-generation     | TensorFlow | https://huggingface.co/t5-base                                                |\n",
    "| gpt2                                                  | text-generation          | PyTorch    | https://huggingface.co/gpt2                                                   |\n",
    "| gpt2                                                  | text-generation          | TensorFlow | https://huggingface.co/gpt2                                                   |\n",
    "| google/tapas-base-finetuned-wtq                       | table-question-answering | PyTorch    | https://huggingface.co/google/tapas-base-finetuned-wtq                        |\n",
    "| bert-large-uncased-whole-word-masking-finetuned-squad | question-answering       | PyTorch    | https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad  |\n",
    "| nlptown/bert-base-multilingual-uncased-sentiment      | text-classification      | PyTorch    | https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment       |\n",
    "| ProsusAI/finbert                                      | text-classification      | PyTorch    | https://huggingface.co/ProsusAI/finbert                                       |\n",
    "| mrm8488/bert-mini-finetuned-age_news-classification   | text-classification      | PyTorch    | https://huggingface.co/mrm8488/bert-mini-finetuned-age_news-classification    |\n",
    "| typeform/distilbert-base-uncased-mnli                 | zero-shot-classification | PyTorch    | https://huggingface.co/typeform/distilbert-base-uncased-mnli                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-transformation",
   "metadata": {},
   "source": [
    "## Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to correct version when merged\n",
    "#!pip install sagemaker>=2.46.0 \n",
    "!pip install git+https://github.com/icywang86rui/sagemaker-python-sdk.git@hf-inference --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polar-shower",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "present-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::558105141721:role/sagemaker_execution_role\n",
      "sagemaker bucket: sagemaker-us-east-1-558105141721\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "# # âš ï¸ local config âš ï¸ (comment in if you run this notebook outside of SageMaker)\n",
    "# import boto3\n",
    "\n",
    "# iam_client = boto3.client('iam')\n",
    "# # role = iam_client.get_role(RoleName='role-name-of-your-iam-role-with-right-permissions')['Role']['Arn']\n",
    "# role = iam_client.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "# sess = sagemaker.Session()    \n",
    "\n",
    "# âš ï¸ SageMaker config âš ï¸ (comment out if you run this notebook outside of SageMaker)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-captain",
   "metadata": {},
   "source": [
    "## Deploy a trained Hugging Face Transformer model on to SageMaker for inference\n",
    "\n",
    "There are two ways on how you can deploy you SageMaker trained Hugging Face model. You can either deploy it after your training is finished or you can deploy it later using the `model_data` pointing to you saved model on s3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-israeli",
   "metadata": {},
   "source": [
    "### Deploy the model directly after training\n",
    "If you deploy you model directly after training you need to make sure that all required files are saved in your training script, including the Tokenizer and the Model. \n",
    "```python\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "############ pseudo code start ############\n",
    "\n",
    "# create HuggingFace estimator for running training\n",
    "huggingface_estimator = HuggingFace(....)\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(...)\n",
    "\n",
    "############ pseudo code end ############\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = hf_estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strategic-resident",
   "metadata": {},
   "source": [
    "### Deploy the model using `model_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fifteen-estimate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.5001324415206909}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   model_data=\"s3://sagemaker-us-east-1-558105141721/sagemaker-huggingface-serving/models/model.tar.gz\",  # path to your trained sagemaker model\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   image_uri=pytorch_cpu_image_uri, # Beta DLC image uri\n",
    "   #transformers_version=\"4.6\", # transformers version used\n",
    "   #pytorch_version=\"1.7\", # pytorch version used\n",
    ")\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "\n",
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "   \"inputs\": \"Camera - You are awarded a SiPix Digital Camera! call 09061221066 fromm landline. Delivery within 28 days.\"\n",
    "}\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compliant-object",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-metro",
   "metadata": {},
   "source": [
    "## Deploy one of the 10 000+ Hugging Face Transformers to Amazon SageMaker for Inference\n",
    "\n",
    "_This is an experimental feature, where the model will be loaded after the endpoint is created. This could lead to errors, e.g. models > 10GB_\n",
    "\n",
    "To deploy a model directly from the Hub to SageMaker we need to define 2 environment variables when creating the `HuggingFaceModel` . We need to define:\n",
    "\n",
    "- `HF_MODEL_ID`: defines the model id, which will be automatically loaded from [huggingface.co/models](http://huggingface.co/models) when creating or SageMaker Endpoint. The ðŸ¤— Hub provides +10 000 models all available through this environment variable.\n",
    "- `HF_TASK`: defines the task for the used ðŸ¤— Transformers pipeline. A full list of tasks can be find [here](https://huggingface.co/transformers/main_classes/pipelines.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "physical-macedonia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.9987210035324097, 'start': 68, 'end': 77, 'answer': 'sagemaker'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n",
    "  'HF_TASK':'question-answering' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   image_uri=pytorch_cpu_image_uri\", # Beta DLC image uri\n",
    "   #transformers_version=\"4.6\", # transformers version used\n",
    "   #pytorch_version=\"1.7\", # pytorch version used\n",
    ")\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "# example request, you always need to define \"inputs\"\n",
    "data = {\n",
    "\"inputs\": {\n",
    "\t\"question\": \"What is used for inference?\",\n",
    "\t\"context\": \"My Name is Philipp and I live in Nuremberg. This model is used with sagemaker for inference.\"\n",
    "\t}\n",
    "}\n",
    "\n",
    "\n",
    "# request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "gross-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# delete endpoint\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
